{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26009f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ---------- 0) Data ----------\n",
    "MNIST_ROOT = os.getenv(\"MNIST_ROOT\", \"./data\")\n",
    "tx = transforms.ToTensor()\n",
    "try:\n",
    "    train_ds = datasets.MNIST(MNIST_ROOT, train=True,  download=True, transform=tx)\n",
    "    test_ds  = datasets.MNIST(MNIST_ROOT, train=False, download=True, transform=tx)\n",
    "except Exception:\n",
    "    train_ds = datasets.MNIST(MNIST_ROOT, train=True,  download=False, transform=tx)\n",
    "    test_ds  = datasets.MNIST(MNIST_ROOT, train=False, download=False, transform=tx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Model (float) + training ----------\n",
    "class MNISTTiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(1, 8, 3, 1, 1, bias=True)\n",
    "        self.c2 = nn.Conv2d(8, 32, 3, 1, 1, bias=True)\n",
    "        self.maxpool = nn.MaxPool2d(2,2)\n",
    "        self.avgpool = nn.AvgPool2d(2,2)\n",
    "        self.fc = nn.Linear(32*3*3, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.c1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.c2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train_one_epoch(m, opt, loader):\n",
    "    m.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = m(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        loss_sum += loss.item()*xb.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(m, loader):\n",
    "    m.eval()\n",
    "    total, correct = 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = m(xb).argmax(1)\n",
    "        correct += (pred==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return correct/total\n",
    "\n",
    "model = MNISTTiny().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 10\n",
    "for ep in range(EPOCHS):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, opt, train_loader)\n",
    "    te_acc = eval_acc(model, test_loader)\n",
    "    print(f\"Epoch {ep+1}/{EPOCHS}: train_loss={tr_loss:.4f}  train_acc={tr_acc:.4f}  test_acc={te_acc:.4f}\")\n",
    "\n",
    "baseline_acc = eval_acc(model, test_loader)\n",
    "print(\"Baseline float32 test accuracy:\", baseline_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) PTQ (symmetric int8) ----------\n",
    "@torch.no_grad()\n",
    "def collect_act_ranges(m, loader, n_batches=50):\n",
    "    m.eval()\n",
    "    a1_max = 0.0  # after ReLU(c1)\n",
    "    a2_max = 0.0  # after ReLU(c2)\n",
    "    a3_max = 0.0  # after avgpool (input to fc)\n",
    "    cnt = 0\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        x = F.relu(m.c1(xb))\n",
    "        a1_max = max(a1_max, x.abs().amax().item())\n",
    "        x = m.maxpool(x)\n",
    "        x = F.relu(m.c2(x))\n",
    "        a2_max = max(a2_max, x.abs().amax().item())\n",
    "        x = m.maxpool(x)\n",
    "        x = m.avgpool(x)\n",
    "        a3_max = max(a3_max, x.abs().amax().item())\n",
    "        cnt += 1\n",
    "        if cnt >= n_batches:\n",
    "            break\n",
    "    return a1_max, a2_max, a3_max\n",
    "\n",
    "def calc_scale(max_abs):\n",
    "    max_abs = max(max_abs, 1e-8)\n",
    "    return max_abs / 127.0\n",
    "\n",
    "a1_max, a2_max, a3_max = collect_act_ranges(model, train_loader, n_batches=50)\n",
    "sx1 = calc_scale(a1_max)\n",
    "sx2 = calc_scale(a2_max)\n",
    "sx3 = calc_scale(a3_max)\n",
    "\n",
    "def per_channel_w_scale(W):\n",
    "    W_ = W.detach().float().cpu()\n",
    "    if W_.dim() == 4:\n",
    "        out_abs = W_.abs().amax(dim=(1,2,3))\n",
    "    else:\n",
    "        out_abs = W_.abs().amax(dim=1)\n",
    "    out_abs = torch.clamp(out_abs, min=1e-8)\n",
    "    return (out_abs / 127.0).numpy()  # [out]\n",
    "\n",
    "sc1 = per_channel_w_scale(model.c1.weight)\n",
    "sc2 = per_channel_w_scale(model.c2.weight)\n",
    "sf  = per_channel_w_scale(model.fc.weight)\n",
    "\n",
    "def quant_w_per_out(W, s_out):\n",
    "    Wf = W.detach().cpu().float().numpy()\n",
    "    Q = np.zeros_like(Wf, dtype=np.int8)\n",
    "    if Wf.ndim == 4:\n",
    "        for o in range(Wf.shape[0]):\n",
    "            Q[o] = np.clip(np.round(Wf[o] / s_out[o]), -128, 127).astype(np.int32).astype(np.int8)\n",
    "    else:\n",
    "        for o in range(Wf.shape[0]):\n",
    "            Q[o] = np.clip(np.round(Wf[o] / s_out[o]), -128, 127).astype(np.int32).astype(np.int8)\n",
    "    return Q\n",
    "\n",
    "Wc1_q     = quant_w_per_out(model.c1.weight, sc1)\n",
    "Wc2_q     = quant_w_per_out(model.c2.weight, sc2)\n",
    "Wf_qO_in  = quant_w_per_out(model.fc.weight,  sf)\n",
    "Wf_q      = (Wf_qO_in.transpose(1,0)).copy()\n",
    "\n",
    "# Bias quantization: int32, scale = s_in * s_w[out]\n",
    "s_x0 = 1.0 / 127.0\n",
    "b1_q = np.round(model.c1.bias.detach().cpu().numpy() / (s_x0 * sc1)).astype(np.int32)\n",
    "b2_q = np.round(model.c2.bias.detach().cpu().numpy() / (sx1  * sc2)).astype(np.int32)\n",
    "bf_q = np.round(model.fc.bias.detach().cpu().numpy()  / (sx3  * sf )).astype(np.int32)\n",
    "\n",
    "# Target output activation scales\n",
    "sy1 = sx1\n",
    "sy2 = sx2\n",
    "sy3 = sx3\n",
    "sy_logits = 0.5\n",
    "\n",
    "def make_requant_params(s_in, s_w_out, s_out):\n",
    "    M_real = (s_in * s_w_out) / s_out\n",
    "    M = np.zeros_like(s_w_out, dtype=np.int32)\n",
    "    S = np.zeros_like(s_w_out, dtype=np.int32)\n",
    "    for i, m in enumerate(M_real):\n",
    "        if m <= 0:\n",
    "            M[i], S[i] = 0, 0\n",
    "            continue\n",
    "        S_i = max(0, 31 - int(np.floor(np.log2(m))))\n",
    "        found = False\n",
    "        for dS in range(0, 32):\n",
    "            S_try = S_i + dS\n",
    "            Mi = int(round(m * (1 << S_try)))\n",
    "            if 0 < Mi < (1 << 31):\n",
    "                M[i], S[i] = Mi, S_try\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            M[i], S[i] = int(m * (1 << 30)), 30\n",
    "    return M, S\n",
    "\n",
    "M1, S1 = make_requant_params(s_x0, sc1, sy1)\n",
    "M2, S2 = make_requant_params(sx1,  sc2, sy2)\n",
    "Mf, Sf = make_requant_params(sx3,  sf,  sy_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3) Save params ----------\n",
    "os.makedirs(\"quant_params\", exist_ok=True)\n",
    "np.save(\"quant_params/FILTER_conv0_int8.npy\", Wc1_q)\n",
    "np.save(\"quant_params/FILTER_conv1_int8.npy\", Wc2_q)\n",
    "np.save(\"quant_params/WEIGHT_fc0_int8_IO.npy\", Wf_q)\n",
    "np.save(\"quant_params/WEIGHT_fc0_int8_OI.npy\", Wf_qO_in)\n",
    "\n",
    "np.save(\"quant_params/BIAS_conv0_int32.npy\", b1_q)\n",
    "np.save(\"quant_params/BIAS_conv1_int32.npy\", b2_q)\n",
    "np.save(\"quant_params/BIAS_fc0_int32.npy\",  bf_q)\n",
    "\n",
    "np.save(\"quant_params/M1_int32.npy\", M1); np.save(\"quant_params/S1_int32.npy\", S1)\n",
    "np.save(\"quant_params/M2_int32.npy\", M2); np.save(\"quant_params/S2_int32.npy\", S2)\n",
    "np.save(\"quant_params/Mf_int32.npy\", Mf); np.save(\"quant_params/Sf_int32.npy\", Sf)\n",
    "\n",
    "print(\"Saved quantized parameters to ./quant_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e18b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
